{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIXiO0sQ5Zbr"
      },
      "source": [
        "Run the following command to clone the repository in the current Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXr9hf9LCbeb",
        "outputId": "254d3dc6-76ff-4c42-a932-e089508025f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jcorsetti/NLU-SpanSA.git\n",
        "!mv NLU-SpanSA/* .\n",
        "!rm -r NLU-SpanSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHvM_Lh25hwR"
      },
      "source": [
        "Install the requirements and check given GPU. Note that for BERT-Large with bs=32 (default config) at least 16Gb on the GPU are necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybx3ywo_CyVk"
      },
      "outputs": [],
      "source": [
        "\n",
        "!mkdir experiments\n",
        "!pip install -r requirements.txt\n",
        "!pip install --upgrade google-cloud-storage\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the below cell to download the pretrained BERT-Large from https://huggingface.co/bert-large-uncased/blob/main/pytorch_model.bin. The script will place the file in the /bert_models/bert-large-uncased folder. This may take some time due to the dimension of the downloaded files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git lfs clone https://huggingface.co/bert-large-uncased\n",
        "!mv bert-large-uncased/pytorch_model.bin bert_models/bert-large-uncased/\n",
        "!rm -r bert-large-uncased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heZsuZOO5zvV"
      },
      "source": [
        "Run the cell below to perform a single training. The settings can be customized in the config.ini file. A default training with 3 epochs and bs=32 takes about 4 minutes. When running an experiment, the config.ini is copied in the exp folder to make it reproducible. Also a predictions.json file with the predicted spans and classes is produced and saved in the experiments folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl1cMKYW3-nW",
        "outputId": "d03f8cf1-7027-43db-d15e-b36277d05480"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from pretrained checkpoint : bert_models/bert-large-uncased/pytorch_model.bin\n",
            "Filtering sentences with no aspects.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/dataset.py:205: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  all_input_ids = torch.tensor([f['subtokens_id'] for f in feat_dataset], dtype=torch.long)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering sentences with no aspects.\n",
            "Train samples:  1458\n",
            "Test samples:  411\n",
            "Num steps:  136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/bert/optimization.py:131: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:    9, Loss: 5.1825 Span loss: 3.7695 Class loss: 1.4130\n",
            "Step:   18, Loss: 3.8114 Span loss: 2.5959 Class loss: 1.2155\n",
            "Step:   27, Loss: 3.1238 Span loss: 2.0329 Class loss: 1.0909\n",
            "Step:   36, Loss: 2.7265 Span loss: 1.7278 Class loss: 0.9987\n",
            "Step:   45, Loss: 2.4801 Span loss: 1.5529 Class loss: 0.9272\n",
            "Step:   54, Loss: 1.0782 Span loss: 0.5961 Class loss: 0.4821\n",
            "Step:   63, Loss: 1.1243 Span loss: 0.6200 Class loss: 0.5043\n",
            "Step:   72, Loss: 1.0918 Span loss: 0.6318 Class loss: 0.4600\n",
            "General metrics --> P: 64.60 R: 57.57 F1: 60.88 Common:  365 Retrieved:  565 Relevant:  634\n",
            "Step:   81, Loss: 1.0746 Span loss: 0.6259 Class loss: 0.4487\n",
            "General metrics --> P: 66.32 R: 60.57 F1: 63.31 Common:  384 Retrieved:  579 Relevant:  634\n",
            "Step:   90, Loss: 1.0864 Span loss: 0.6821 Class loss: 0.4043\n",
            "General metrics --> P: 64.13 R: 61.20 F1: 62.63 Common:  388 Retrieved:  605 Relevant:  634\n",
            "Step:   99, Loss: 0.7535 Span loss: 0.5054 Class loss: 0.2481\n",
            "General metrics --> P: 63.42 R: 58.52 F1: 60.87 Common:  371 Retrieved:  585 Relevant:  634\n",
            "Step:  108, Loss: 0.8406 Span loss: 0.5138 Class loss: 0.3268\n",
            "General metrics --> P: 64.67 R: 58.04 F1: 61.18 Common:  368 Retrieved:  569 Relevant:  634\n",
            "Step:  117, Loss: 0.6845 Span loss: 0.4356 Class loss: 0.2489\n",
            "General metrics --> P: 64.10 R: 59.15 F1: 61.53 Common:  375 Retrieved:  585 Relevant:  634\n",
            "Step:  126, Loss: 0.7657 Span loss: 0.5178 Class loss: 0.2479\n",
            "General metrics --> P: 63.11 R: 59.62 F1: 61.31 Common:  378 Retrieved:  599 Relevant:  634\n",
            "Step:  135, Loss: 0.6791 Span loss: 0.4498 Class loss: 0.2293\n",
            "General metrics --> P: 63.23 R: 59.94 F1: 61.54 Common:  380 Retrieved:  601 Relevant:  634\n",
            "General metrics --> P: 63.23 R: 59.94 F1: 61.54 Common:  380 Retrieved:  601 Relevant:  634\n",
            "Span metrics --> P: 81.03 R: 76.81 F1: 78.87\n",
            "Class metrics --> Accuracy: 78.23 Correct:  496 Total:  634\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import subprocess\n",
        "from os.path import join\n",
        "from configparser import ConfigParser\n",
        "from utils import bert_load_state_dict, init_seed, get_optimizer, span_bound2position\n",
        "from evaluation import evaluate, evaluate_spanonly, evaluate_classonly\n",
        "from jointBert import JointBert\n",
        "from bert.modeling import BertConfig\n",
        "from dataset import get_dataloader\n",
        "from losses import compute_class_loss, compute_span_loss\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "DEV='cuda'\n",
        "#DICT = {'OTHER': 0, 'T-NEU': 1, 'T-POS': 2, 'T-NEG': 3, 'CONF' : 4}\n",
        "DICT = {'T-NEU': 0, 'T-POS': 1, 'T-NEG': 2}\n",
        "config = ConfigParser()\n",
        "config.read('config.ini')\n",
        "\n",
        "seed = int(config['data']['seed'])\n",
        "init_seed(seed)\n",
        "\n",
        "exp_root_dir = join('experiments', config['data']['exp_name'])\n",
        "if not os.path.isdir(exp_root_dir): \n",
        "    subprocess.call('mkdir {}'.format(exp_root_dir), shell=True)\n",
        "subprocess.call('cp config.ini {}/'.format(exp_root_dir), shell=True)\n",
        "\n",
        "log_file = join(exp_root_dir,'log.txt')\n",
        "if os.path.exists(log_file):\n",
        "    print('Warning: overwriting previous logfile {}'.format(log_file))\n",
        "    subprocess.call('rm {}'.format(log_file), shell=True)\n",
        "\n",
        "bert_model = join(config['arch']['root'],config['arch']['bert'])\n",
        "bert_config = BertConfig.from_json_file(join(bert_model,'bert_config.json'))\n",
        "\n",
        "model = JointBert(bert_config, config['arch']['span_head'], config['arch']['polarity_head'])\n",
        "model = bert_load_state_dict(model, torch.load(join(bert_model, 'pytorch_model.bin'), map_location='cpu'))\n",
        "model = model.to(DEV)\n",
        "print(\"Loading model from pretrained checkpoint : {}\".format(join(bert_model, 'pytorch_model.bin')))\n",
        "\n",
        "BS, N_EPOCHS = int(config['training']['batch_size']), int(config['training']['epochs'])\n",
        "MAX_SEQ_LEN = int(config['arch']['max_sequence_lenght'])\n",
        "M1,M2 = float(config['training']['span_loss_w']), float(config['training']['class_loss_w'])\n",
        "filter_empty_train = config.getboolean('data','filter_empty_train')\n",
        "filter_empty_valid = config.getboolean('data','filter_empty_valid')\n",
        "\n",
        "train_dataloader, train_feats, num_train_samples = get_dataloader(config['data']['train_file'], DICT, filter_empty_train, config)\n",
        "test_dataloader, test_feats, num_test_samples = get_dataloader(config['data']['valid_file'], DICT, filter_empty_valid, config)\n",
        "\n",
        "print('Train samples: ', num_train_samples)\n",
        "print('Test samples: ', num_test_samples)\n",
        "\n",
        "# calculating save intervals like in original paper\n",
        "num_steps = int(num_train_samples / BS * N_EPOCHS)\n",
        "save_cp_interval = int(num_steps/ (5*N_EPOCHS))\n",
        "start_save_steps = int(num_steps * 0.5)\n",
        "\n",
        "optimizer = get_optimizer(model, config['training'], num_steps)\n",
        "\n",
        "print('Num steps: ', num_steps)\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    running_loss, count = 0., 0\n",
        "    running_span, running_class = 0.,0.\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        batch = tuple(t.to(DEV) for t in batch)  \n",
        "        input_ids, input_mask, segment_ids, start_span, end_span, polarity, polarity_mask, example_indices = batch\n",
        "\n",
        "        # produce spans from list of index to mask format, used for loss\n",
        "        start_positions = span_bound2position(start_span, polarity_mask, MAX_SEQ_LEN)\n",
        "        end_positions = span_bound2position(end_span, polarity_mask, MAX_SEQ_LEN)\n",
        "        start_logits, end_logits, pol_logits = model.forward(input_ids, input_mask, segment_ids, start_span, end_span)\n",
        "\n",
        "        # calculate losses\n",
        "        span_loss = compute_span_loss(start_logits, end_logits, start_positions, end_positions)\n",
        "        class_loss = compute_class_loss(pol_logits, polarity, polarity_mask)\n",
        "\n",
        "        cur_loss = M1*span_loss + M2*class_loss\n",
        "\n",
        "        # backward pass\n",
        "        cur_loss.backward()\n",
        "        running_loss += cur_loss.item()\n",
        "        running_span += M1*span_loss.item()\n",
        "        running_class += M2*class_loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "        global_step += 1\n",
        "        count += 1\n",
        "\n",
        "        # Each n steps print the loss\n",
        "        if global_step % save_cp_interval == 0 and count > 0:\n",
        "            print(\"Step: {:4d}, Loss: {:.4f} Span loss: {:.4f} Class loss: {:.4f}\".format(global_step, running_loss / count, running_span / count, running_class / count))\n",
        "            with open(log_file,'a') as f:\n",
        "                print(\"Step: {:4d}, Loss: {:.4f} Span loss: {:.4f} Class loss: {:.4f}\".format(global_step, running_loss / count, running_span / count, running_class / count), file=f)\n",
        "\n",
        "            # also, if after half steps, start saving best checkpoint\n",
        "            if global_step > start_save_steps:\n",
        "                model.eval()\n",
        "\n",
        "                p,r,f1,common,retrieved,relevant, _ = evaluate(model, test_dataloader, test_feats, DICT, config, DEV)\n",
        "                toprint = 'General metrics --> P: {:2.2f} R: {:2.2f} F1: {:2.2f} Common: {:4d} Retrieved: {:4d} Relevant: {:4d}'.format(p*100,r*100,f1*100,int(common),int(retrieved),int(relevant)) \n",
        "                print(toprint)\n",
        "                with open(log_file,'a') as f:\n",
        "                    print(toprint,file=f)\n",
        "\n",
        "                running_loss, running_span, running_class, count = 0., 0., 0., 0\n",
        "                model.train()\n",
        "\n",
        "                torch.save({\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'step': global_step\n",
        "                }, join(exp_root_dir, 'checkpoint_last.pth'))\n",
        "\n",
        "\n",
        "# run final evaluation, make descriptive strings of metrics to be saved\n",
        "p,r,f1,common,retrieved,relevant,preds = evaluate(model, test_dataloader, test_feats, DICT, config, DEV)\n",
        "toprint_all = 'General metrics --> P: {:2.2f} R: {:2.2f} F1: {:2.2f} Common: {:4d} Retrieved: {:4d} Relevant: {:4d}'.format(p*100,r*100,f1*100,int(common),int(retrieved),int(relevant)) \n",
        "p,r,f1, = evaluate_spanonly(model, test_dataloader, test_feats, config, DEV)\n",
        "toprint_span = 'Span metrics --> P: {:2.2f} R: {:2.2f} F1: {:2.2f}'.format(p*100,r*100,f1*100) \n",
        "accuracy, correct, total = evaluate_classonly(model, test_dataloader, test_feats, config, DEV)\n",
        "toprint_class = 'Class metrics --> Accuracy: {:2.2f} Correct: {:4d} Total: {:4d}'.format(accuracy*100, int(correct), int(total))\n",
        "\n",
        "print(toprint_all)\n",
        "print(toprint_span)\n",
        "print(toprint_class)\n",
        "\n",
        "if log_file is not None:\n",
        "    with open(log_file,'a') as f:\n",
        "        print(toprint_all,file=f)\n",
        "        print(toprint_span,file=f)\n",
        "        print(toprint_class,file=f)\n",
        "\n",
        "\n",
        "with open(join(exp_root_dir, 'predictions.json'),'w') as f:\n",
        "    json.dump(preds, f)\n",
        "#print('Epoch {:2d} : \\tTotal: {:.4f}\\tClass: {:.4f}\\tSpan: {:.4f}'.format(epoch, running_loss / step_count,running_class / step_count,running_span / step_count))    \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR9VlQ-76FKn"
      },
      "source": [
        "The following cell has been used to produce the metrics reported in the report. 5 independent experiments with random seed are run, and the final metrics are averaged. Note that the checkpoints and predictions are not saved in this case, and the evaluation is performed only at the end to save time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fErIigZOx_zl",
        "outputId": "8051f615-2d2c-4044-b9e5-cada6ac78a8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering sentences with no aspects.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/dataset.py:205: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  all_input_ids = torch.tensor([f['subtokens_id'] for f in feat_dataset], dtype=torch.long)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtering sentences with no aspects.\n",
            "Train samples:  1458\n",
            "Test samples:  411\n",
            "3626\n",
            "Repetition 1 of 3\n",
            "Loading model from pretrained checkpoint : bert_models/bert-large-uncased/pytorch_model.bin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/bert/optimization.py:131: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:    9, Loss: 5.2931 Span loss: 3.9359 Class loss: 1.3572\n",
            "Step:   18, Loss: 3.8560 Span loss: 2.6709 Class loss: 1.1851\n",
            "Step:   27, Loss: 3.1200 Span loss: 2.0673 Class loss: 1.0527\n",
            "Step:   36, Loss: 2.7151 Span loss: 1.7436 Class loss: 0.9716\n",
            "Step:   45, Loss: 2.4418 Span loss: 1.5402 Class loss: 0.9016\n",
            "Step:   54, Loss: 1.0059 Span loss: 0.5787 Class loss: 0.4272\n",
            "Step:   63, Loss: 0.9868 Span loss: 0.5553 Class loss: 0.4315\n",
            "Step:   72, Loss: 0.9857 Span loss: 0.5700 Class loss: 0.4157\n",
            "Step:   81, Loss: 0.9740 Span loss: 0.5520 Class loss: 0.4220\n",
            "Step:   90, Loss: 0.9512 Span loss: 0.5613 Class loss: 0.3899\n",
            "Step:   99, Loss: 0.6042 Span loss: 0.4113 Class loss: 0.1929\n",
            "Step:  108, Loss: 0.7689 Span loss: 0.4947 Class loss: 0.2741\n",
            "Step:  117, Loss: 0.6322 Span loss: 0.4278 Class loss: 0.2044\n",
            "Step:  126, Loss: 0.6700 Span loss: 0.4657 Class loss: 0.2043\n",
            "Step:  135, Loss: 0.6039 Span loss: 0.4419 Class loss: 0.1620\n",
            "5882\n",
            "Repetition 2 of 3\n",
            "Loading model from pretrained checkpoint : bert_models/bert-large-uncased/pytorch_model.bin\n",
            "Step:    9, Loss: 5.1218 Span loss: 3.6785 Class loss: 1.4433\n",
            "Step:   18, Loss: 3.7992 Span loss: 2.5535 Class loss: 1.2456\n",
            "Step:   27, Loss: 3.1336 Span loss: 2.0113 Class loss: 1.1223\n",
            "Step:   36, Loss: 2.7621 Span loss: 1.7332 Class loss: 1.0289\n",
            "Step:   45, Loss: 2.4760 Span loss: 1.5272 Class loss: 0.9488\n",
            "Step:   54, Loss: 1.0941 Span loss: 0.6090 Class loss: 0.4852\n",
            "Step:   63, Loss: 1.1240 Span loss: 0.5911 Class loss: 0.5328\n",
            "Step:   72, Loss: 1.1347 Span loss: 0.5916 Class loss: 0.5431\n",
            "Step:   81, Loss: 1.0143 Span loss: 0.5754 Class loss: 0.4389\n",
            "Step:   90, Loss: 1.0598 Span loss: 0.6067 Class loss: 0.4531\n",
            "Step:   99, Loss: 0.8741 Span loss: 0.5192 Class loss: 0.3549\n",
            "Step:  108, Loss: 0.7359 Span loss: 0.4616 Class loss: 0.2743\n",
            "Step:  117, Loss: 0.7034 Span loss: 0.4562 Class loss: 0.2472\n",
            "Step:  126, Loss: 0.7411 Span loss: 0.4969 Class loss: 0.2442\n",
            "Step:  135, Loss: 0.7347 Span loss: 0.4784 Class loss: 0.2563\n",
            "6709\n",
            "Repetition 3 of 3\n",
            "Loading model from pretrained checkpoint : bert_models/bert-large-uncased/pytorch_model.bin\n",
            "Step:    9, Loss: 5.1553 Span loss: 3.7376 Class loss: 1.4177\n",
            "Step:   18, Loss: 3.8123 Span loss: 2.5894 Class loss: 1.2228\n",
            "Step:   27, Loss: 3.1337 Span loss: 2.0256 Class loss: 1.1081\n",
            "Step:   36, Loss: 2.7116 Span loss: 1.7087 Class loss: 1.0030\n",
            "Step:   45, Loss: 2.4734 Span loss: 1.5178 Class loss: 0.9556\n",
            "Step:   54, Loss: 1.1486 Span loss: 0.7004 Class loss: 0.4483\n",
            "Step:   63, Loss: 1.1206 Span loss: 0.6242 Class loss: 0.4963\n",
            "Step:   72, Loss: 1.0978 Span loss: 0.6055 Class loss: 0.4922\n",
            "Step:   81, Loss: 1.4405 Span loss: 0.8868 Class loss: 0.5536\n",
            "Step:   90, Loss: 1.0809 Span loss: 0.6489 Class loss: 0.4320\n",
            "Step:   99, Loss: 0.8906 Span loss: 0.5588 Class loss: 0.3318\n",
            "Step:  108, Loss: 0.8389 Span loss: 0.5449 Class loss: 0.2940\n",
            "Step:  117, Loss: 0.7931 Span loss: 0.4660 Class loss: 0.3271\n",
            "Step:  126, Loss: 0.7382 Span loss: 0.4745 Class loss: 0.2637\n",
            "Step:  135, Loss: 0.8066 Span loss: 0.4851 Class loss: 0.3215\n",
            "Final metrics\n",
            "p_all : 66.08 (+- 0.908)\n",
            "r_all : 63.04 (+- 0.520)\n",
            "f1_all : 64.52 (+- 0.230)\n",
            "p_span : 82.82 (+- 0.723)\n",
            "r_span : 79.02 (+- 1.022)\n",
            "f1_span : 80.87 (+- 0.263)\n",
            "acc_class : 78.55 (+- 0.446)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from os.path import join\n",
        "from configparser import ConfigParser\n",
        "from utils import bert_load_state_dict, init_seed, get_optimizer, span_bound2position\n",
        "from evaluation import evaluate, evaluate_spanonly, evaluate_classonly\n",
        "from jointBert import JointBert\n",
        "from bert.modeling import BertConfig\n",
        "from dataset import get_dataloader\n",
        "from losses import compute_class_loss, compute_span_loss\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "DEV='cuda'\n",
        "#DICT = {'OTHER': 0, 'T-NEU': 1, 'T-POS': 2, 'T-NEG': 3, 'CONF' : 4}\n",
        "DICT = {'T-NEU': 0, 'T-POS': 1, 'T-NEG': 2}\n",
        "config = ConfigParser()\n",
        "config.read('config.ini')\n",
        "\n",
        "\n",
        "\n",
        "exp_root_dir = join('experiments', config['data']['exp_name'])\n",
        "if not os.path.isdir(exp_root_dir): \n",
        "    subprocess.call('mkdir {}'.format(exp_root_dir), shell=True)\n",
        "subprocess.call('cp config.ini {}/'.format(exp_root_dir), shell=True)\n",
        "\n",
        "log_file = join(exp_root_dir,'log.txt')\n",
        "if os.path.exists(log_file):\n",
        "    print('Warning: overwriting previous logfile {}'.format(log_file))\n",
        "    subprocess.call('rm {}'.format(log_file), shell=True)\n",
        "\n",
        "\n",
        "\n",
        "BS, N_EPOCHS = int(config['training']['batch_size']), int(config['training']['epochs'])\n",
        "MAX_SEQ_LEN = int(config['arch']['max_sequence_lenght'])\n",
        "M1,M2 = float(config['training']['span_loss_w']), float(config['training']['class_loss_w'])\n",
        "filter_empty_train = config.getboolean('data','filter_empty_train')\n",
        "filter_empty_valid = config.getboolean('data','filter_empty_valid')\n",
        "\n",
        "train_dataloader, train_feats, num_train_samples = get_dataloader(config['data']['train_file'], DICT, filter_empty_train, config)\n",
        "test_dataloader, test_feats, num_test_samples = get_dataloader(config['data']['valid_file'], DICT, filter_empty_valid, config)\n",
        "\n",
        "print('Train samples: ', num_train_samples)\n",
        "print('Test samples: ', num_test_samples)\n",
        "\n",
        "# calculating save intervals like in original paper\n",
        "num_steps = int(num_train_samples / BS * N_EPOCHS)\n",
        "save_cp_interval = int(num_steps/ (5*N_EPOCHS))\n",
        "start_save_steps = int(num_steps * 0.5)\n",
        "\n",
        "NUM_REPETITIONS = 3 # this can be changed for stabler results\n",
        "\n",
        "all_metrics = {\n",
        "    'p_all': [],\n",
        "    'r_all': [],\n",
        "    'f1_all': [],\n",
        "    'p_span': [],\n",
        "    'r_span': [],\n",
        "    'f1_span': [],\n",
        "    'acc_class': []\n",
        "}\n",
        "\n",
        "for repeat in range(NUM_REPETITIONS):\n",
        "\n",
        "    seed = int(10000*random.random())\n",
        "    print(seed)\n",
        "    init_seed(seed)\n",
        "\n",
        "    print('Repetition {} of {}'.format(repeat+1, NUM_REPETITIONS))\n",
        "    bert_model = join(config['arch']['root'],config['arch']['bert'])\n",
        "    bert_config = BertConfig.from_json_file(join(bert_model,'bert_config.json'))\n",
        "\n",
        "    model = JointBert(bert_config, config['arch']['span_head'], config['arch']['polarity_head'])\n",
        "    model = bert_load_state_dict(model, torch.load(join(bert_model, 'pytorch_model.bin'), map_location='cpu'))\n",
        "    model = model.to(DEV)\n",
        "    print(\"Loading model from pretrained checkpoint : {}\".format(join(bert_model, 'pytorch_model.bin')))\n",
        "\n",
        "    optimizer = get_optimizer(model, config['training'], num_steps)\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "\n",
        "        running_loss, count = 0., 0\n",
        "        running_span, running_class = 0.,0.\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            batch = tuple(t.to(DEV) for t in batch)  \n",
        "            input_ids, input_mask, segment_ids, start_span, end_span, polarity, polarity_mask, example_indices = batch\n",
        "\n",
        "            # produce spans from list of index to mask format, used for loss\n",
        "            start_positions = span_bound2position(start_span, polarity_mask, MAX_SEQ_LEN)\n",
        "            end_positions = span_bound2position(end_span, polarity_mask, MAX_SEQ_LEN)\n",
        "            start_logits, end_logits, pol_logits = model.forward(input_ids, input_mask, segment_ids, start_span, end_span)\n",
        "\n",
        "            span_loss = compute_span_loss(start_logits, end_logits, start_positions, end_positions)\n",
        "            class_loss = compute_class_loss(pol_logits, polarity, polarity_mask)\n",
        "\n",
        "            cur_loss = M1*span_loss + M2*class_loss\n",
        "\n",
        "            cur_loss.backward()\n",
        "            running_loss += cur_loss.item()\n",
        "            running_span += M1*span_loss.item()\n",
        "            running_class += M2*class_loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "            global_step += 1\n",
        "            count += 1\n",
        "\n",
        "            # Each n steps print the loss\n",
        "            if global_step % save_cp_interval == 0 and count > 0:\n",
        "                print(\"Step: {:4d}, Loss: {:.4f} Span loss: {:.4f} Class loss: {:.4f}\".format(global_step, running_loss / count, running_span / count, running_class / count))\n",
        "                with open(log_file,'a') as f:\n",
        "                    print(\"Step: {:4d}, Loss: {:.4f} Span loss: {:.4f} Class loss: {:.4f}\".format(global_step, running_loss / count, running_span / count, running_class / count), file=f)\n",
        "\n",
        "                # also, if after half steps, start saving best checkpoint\n",
        "                if global_step > start_save_steps:\n",
        "                    running_loss, running_span, running_class, count = 0., 0., 0., 0\n",
        "\n",
        "    p,r,f1,_,_,_,_ = evaluate(model, test_dataloader, test_feats, DICT, config, DEV)\n",
        "    p_span, r_span, f1_span = evaluate_spanonly(model, test_dataloader, test_feats, config, DEV)\n",
        "    acc, _, _ = evaluate_classonly(model, test_dataloader, test_feats, config, DEV)\n",
        "    all_metrics['p_all'].append(p*100)\n",
        "    all_metrics['r_all'].append(r*100)\n",
        "    all_metrics['f1_all'].append(f1*100)\n",
        "    all_metrics['p_span'].append(p_span*100)\n",
        "    all_metrics['r_span'].append(r_span*100)\n",
        "    all_metrics['f1_span'].append(f1_span*100)\n",
        "    all_metrics['acc_class'].append(acc*100)\n",
        "\n",
        "print('Final metrics')\n",
        "updated_metrics = {}\n",
        "for k, v in all_metrics.items():\n",
        "    updated_metrics[k] = v\n",
        "    array = np.asarray(v)\n",
        "    updated_metrics[k + '_mean'] = np.mean(array)\n",
        "    updated_metrics[k + '_std'] = np.std(array)\n",
        "    print('{} : {:.2f} (+- {:.3f})'.format(k,np.mean(array),np.std(array)))\n",
        "    with open(log_file,'a') as f:\n",
        "        print('{} : {:.2f} (+- {:.3f})'.format(k,np.mean(array),np.std(array)), file=f)\n",
        "\n",
        "with open(join(exp_root_dir,'final_metrics.json'), 'w') as f:\n",
        "    json.dump(updated_metrics, f)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JxhJZT66lKB"
      },
      "source": [
        "Running the following cell evaluate a given experiment. It is enough to change the variable EXP with the path of the desired experiment. In this case, the config used will be the one with which the experiment was performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsY1hemB3Lnw",
        "outputId": "011165fb-4d0a-49a5-f70f-d6a3e0043504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from pretrained checkpoint : experiments/repro-3classes/checkpoint_last.pth\n",
            "Filtering sentences with no aspects.\n",
            "Test samples:  411\n",
            "General metrics --> P: 63.23 R: 59.94 F1: 61.54 Common:  380 Retrieved:  601 Relevant:  634\n",
            "Span metrics --> P: 81.03 R: 76.81 F1: 78.87\n",
            "Class metrics --> Accuracy: 78.23 Correct:  496 Total:  634\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from os.path import join\n",
        "import random\n",
        "import subprocess\n",
        "from jointBert import JointBert\n",
        "from utils import *\n",
        "from configparser import ConfigParser\n",
        "\n",
        "EXP = 'experiments/repro-3classes'\n",
        "CP = 'checkpoint_last.pth'\n",
        "\n",
        "DEV='cuda'\n",
        "#DICT = {'OTHER': 0, 'T-NEU': 1, 'T-POS': 2, 'T-NEG': 3, 'CONF' : 4}\n",
        "DICT = {'T-NEU': 0, 'T-POS': 1, 'T-NEG': 2}\n",
        "\n",
        "config = ConfigParser()\n",
        "config.read(join(EXP, 'config.ini'))\n",
        "\n",
        "seed = int(config['data']['seed'])\n",
        "init_seed(seed)\n",
        "\n",
        "bert_model = join(config['arch']['root'],config['arch']['bert'])\n",
        "bert_config = BertConfig.from_json_file(join(bert_model,'bert_config.json'))\n",
        "filter_empty_valid = config.getboolean('data','filter_empty_valid')\n",
        "\n",
        "model = JointBert(bert_config, config['arch']['polarity_head'], config['arch']['span_head'])\n",
        "checkpoint = torch.load(join(EXP,CP))\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model = model.to(DEV)\n",
        "\n",
        "print(\"Loading model from pretrained checkpoint : {}\".format(join(EXP,CP)))\n",
        "\n",
        "MAX_SEQ_LEN = int(config['arch']['max_sequence_lenght'])\n",
        "test_dataloader, test_feats, num_test_samples = get_dataloader(config['data']['valid_file'], DICT, filter_empty_valid, config)\n",
        "\n",
        "print('Test samples: ', num_test_samples)\n",
        "\n",
        "# run final evaluation, make descriptive strings of metrics to be saved\n",
        "p,r,f1,common,retrieved,relevant,preds = evaluate(model, test_dataloader, test_feats, DICT, config, DEV)\n",
        "toprint_all = 'General metrics --> P: {:2.2f} R: {:2.2f} F1: {:2.2f} Common: {:4d} Retrieved: {:4d} Relevant: {:4d}'.format(p*100,r*100,f1*100,int(common),int(retrieved),int(relevant)) \n",
        "p,r,f1, = evaluate_spanonly(model, test_dataloader, test_feats, config, DEV)\n",
        "toprint_span = 'Span metrics --> P: {:2.2f} R: {:2.2f} F1: {:2.2f}'.format(p*100,r*100,f1*100) \n",
        "accuracy, correct, total = evaluate_classonly(model, test_dataloader, test_feats, config, DEV)\n",
        "toprint_class = 'Class metrics --> Accuracy: {:2.2f} Correct: {:4d} Total: {:4d}'.format(accuracy*100, int(correct), int(total))\n",
        "\n",
        "print(toprint_all)\n",
        "print(toprint_span)\n",
        "print(toprint_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5t0To7g9q1K"
      },
      "source": [
        "Run the following to compute the dataset statistics as reported in the report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBWCWFv19qk7",
        "outputId": "3d0a8d5a-362d-465b-c964-6db16c5d5b02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num sentences: 3045\n",
            "  of which empty: 1587\n",
            "  avg length in tokens: 16.6\n",
            "Num aspects: 2302\n",
            "  neutral: 455\n",
            "  negative: 860\n",
            "  positive: 987\n",
            "  avg per nonempty sentence: 1.6\n",
            "Num sentences: 800\n",
            "  of which empty: 389\n",
            "  avg length in tokens: 14.6\n",
            "Num aspects: 634\n",
            "  neutral: 165\n",
            "  negative: 130\n",
            "  positive: 339\n",
            "  avg per nonempty sentence: 1.5\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!python stats.py 'train'\n",
        "!python stats.py 'test'\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "NLU-SA",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('nlu-proj': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "886138c48365167e511a982083bee394f5237cfa6135dfd3f9756bcda73e1991"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
